{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21482ee9",
   "metadata": {},
   "source": [
    "# Empathetic Chatbot - Transformer from Scratch\n",
    "\n",
    "This notebook implements a Transformer encoder-decoder model from scratch for generating empathetic responses.\n",
    "\n",
    "**Dataset**: Empathetic Dialogues (Facebook AI)\n",
    "\n",
    "**Architecture**: Transformer encoder-decoder (no pretrained weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad680d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ba433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "\n",
    "# For evaluation metrics\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# For data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utils\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d05f2b",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afa495",
   "metadata": {},
   "source": [
    "### Important: Update DATA_PATH\n",
    "\n",
    "**For Kaggle:** Set `DATA_PATH = '/kaggle/input/your-dataset-name/demo.csv'`\n",
    "\n",
    "**For Local:** Set `DATA_PATH = 'demo.csv'` or full path like `'d:/BSSE Notes/Generative AI/Assignment2/demo.csv'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b31de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data paths (update this to your data file path)\n",
    "    DATA_PATH = '/kaggle/input/empathetic-dialogues-facebook-ai/demo.csv'  # Single CSV file\n",
    "    # For local testing, use: 'demo.csv' or full path\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    EMBEDDING_DIM = 256\n",
    "    NUM_HEADS = 4\n",
    "    NUM_ENCODER_LAYERS = 2\n",
    "    NUM_DECODER_LAYERS = 2\n",
    "    FFN_DIM = 512\n",
    "    DROPOUT = 0.1\n",
    "    MAX_SEQ_LEN = 128\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 3e-4\n",
    "    NUM_EPOCHS = 20\n",
    "    GRAD_CLIP = 1.0\n",
    "    \n",
    "    # Special tokens\n",
    "    PAD_TOKEN = '<pad>'\n",
    "    BOS_TOKEN = '<bos>'\n",
    "    EOS_TOKEN = '<eos>'\n",
    "    UNK_TOKEN = '<unk>'\n",
    "    \n",
    "    # Vocabulary\n",
    "    MIN_FREQ = 2\n",
    "    MAX_VOCAB_SIZE = 10000\n",
    "    \n",
    "    # Data split ratios\n",
    "    TRAIN_RATIO = 0.8\n",
    "    VAL_RATIO = 0.1\n",
    "    TEST_RATIO = 0.1\n",
    "\n",
    "config = Config()\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a7fdc",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "This section handles:\n",
    "- Loading the dataset\n",
    "- Text normalization (lowercase, punctuation, whitespace)\n",
    "- Building vocabulary from training data\n",
    "- Train/Val/Test split (80/10/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16783a",
   "metadata": {},
   "source": [
    "### Dataset Format\n",
    "\n",
    "Your data has the following structure:\n",
    "- **Situation**: Background context (e.g., \"I remember going to the fireworks with my best friend\")\n",
    "- **emotion**: Emotion label (e.g., \"sentimental\", \"afraid\", \"proud\", \"faithful\")\n",
    "- **empathetic_dialogues**: Contains conversation in format: `\"Customer :{text}\\nAgent :\"`\n",
    "- **labels**: The agent's response (ground truth)\n",
    "\n",
    "The code will:\n",
    "1. Load single CSV file\n",
    "2. Split into 80% train / 10% validation / 10% test\n",
    "3. Extract customer utterance from `empathetic_dialogues` column\n",
    "4. Format as: `\"Emotion: {emotion} | Situation: {situation} | Customer: {customer} Agent:\"`\n",
    "5. Use `labels` as target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text: lowercase, clean whitespace, normalize punctuation\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Normalize punctuation - keep basic punctuation\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple word tokenization\"\"\"\n",
    "    # Split on whitespace and punctuation but keep punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    return tokens\n",
    "\n",
    "print(\"Text preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path):\n",
    "    \"\"\"Load the empathetic dialogues dataset from a single CSV file\"\"\"\n",
    "    try:\n",
    "        # Try loading the dataset\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"Loaded dataset: {len(df)} samples\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Display sample\n",
    "        print(f\"\\nSample row:\")\n",
    "        print(df.head(1))\n",
    "        \n",
    "        # Split into train/val/test (80/10/10)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # First split: 80% train, 20% temp\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "        \n",
    "        # Second split: split temp into 50/50 for val and test (each 10% of total)\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "        \n",
    "        print(f\"\\nSplit sizes:\")\n",
    "        print(f\"  Train: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Validation: {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Test: {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {data_path}\")\n",
    "        print(\"Using demo.csv from current directory...\")\n",
    "        try:\n",
    "            df = pd.read_csv('demo.csv')\n",
    "            print(f\"Loaded demo.csv: {len(df)} samples\")\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "            val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "            return train_df, val_df, test_df\n",
    "        except:\n",
    "            print(\"Creating minimal sample data for demonstration...\")\n",
    "            sample_data = {\n",
    "                'Situation': ['I remember going to the fireworks with my best friend'] * 10,\n",
    "                'emotion': ['sentimental'] * 10,\n",
    "                'empathetic_dialogues': ['Customer :This was a best friend. I miss her.\\nAgent :'] * 10,\n",
    "                'labels': ['Where has she gone?'] * 10\n",
    "            }\n",
    "            df = pd.DataFrame(sample_data)\n",
    "            train_df = df.copy()\n",
    "            val_df = df.copy()\n",
    "            test_df = df.copy()\n",
    "            return train_df, val_df, test_df\n",
    "\n",
    "# Load dataset\n",
    "train_df, val_df, test_df = load_dataset(config.DATA_PATH)\n",
    "print(f\"\\nDataset ready for processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a16795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary class for token to index mapping\"\"\"\n",
    "    \n",
    "    def __init__(self, pad_token='<pad>', bos_token='<bos>', \n",
    "                 eos_token='<eos>', unk_token='<unk>'):\n",
    "        self.pad_token = pad_token\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        # Initialize with special tokens\n",
    "        self.token2idx = {\n",
    "            pad_token: 0,\n",
    "            bos_token: 1,\n",
    "            eos_token: 2,\n",
    "            unk_token: 3\n",
    "        }\n",
    "        self.idx2token = {v: k for k, v in self.token2idx.items()}\n",
    "        self.token_counts = Counter()\n",
    "        \n",
    "    def build_vocabulary(self, texts, min_freq=2, max_vocab_size=10000):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        # Count all tokens\n",
    "        for text in texts:\n",
    "            tokens = simple_tokenize(normalize_text(text))\n",
    "            self.token_counts.update(tokens)\n",
    "        \n",
    "        # Get most common tokens\n",
    "        most_common = self.token_counts.most_common(max_vocab_size)\n",
    "        \n",
    "        # Add tokens that meet minimum frequency\n",
    "        for token, count in most_common:\n",
    "            if count >= min_freq and token not in self.token2idx:\n",
    "                idx = len(self.token2idx)\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token[idx] = token\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.token2idx)}\")\n",
    "        print(f\"Most common tokens: {most_common[:10]}\")\n",
    "        \n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"Convert text to token indices\"\"\"\n",
    "        tokens = simple_tokenize(normalize_text(text))\n",
    "        indices = []\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            indices.append(self.token2idx[self.bos_token])\n",
    "        \n",
    "        for token in tokens:\n",
    "            indices.append(self.token2idx.get(token, self.token2idx[self.unk_token]))\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            indices.append(self.token2idx[self.eos_token])\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices, skip_special_tokens=True):\n",
    "        \"\"\"Convert token indices back to text\"\"\"\n",
    "        tokens = []\n",
    "        special_tokens = {self.token2idx[self.pad_token], \n",
    "                         self.token2idx[self.bos_token], \n",
    "                         self.token2idx[self.eos_token]}\n",
    "        \n",
    "        for idx in indices:\n",
    "            if skip_special_tokens and idx in special_tokens:\n",
    "                continue\n",
    "            tokens.append(self.idx2token.get(idx, self.unk_token))\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token2idx)\n",
    "\n",
    "print(\"Vocabulary class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad96029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    Prepare X and Y pairs in format:\n",
    "    X: \"Emotion: {emotion} | Situation: {situation} | Customer: {customer_utterance} Agent:\"\n",
    "    Y: \"{agent_reply}\"\n",
    "    \n",
    "    Data format:\n",
    "    - Situation: background context\n",
    "    - emotion: the emotion label\n",
    "    - empathetic_dialogues: contains \"Customer :{text}\\nAgent :\"\n",
    "    - labels: the agent's response\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_customer_utterance(dialogue_text):\n",
    "        \"\"\"Extract customer utterance from empathetic_dialogues column\"\"\"\n",
    "        if pd.isna(dialogue_text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Format: \"Customer :{text}\\nAgent :\"\n",
    "        text = str(dialogue_text)\n",
    "        \n",
    "        # Extract text between \"Customer :\" and \"Agent :\"\n",
    "        if \"Customer :\" in text and \"Agent :\" in text:\n",
    "            start = text.index(\"Customer :\") + len(\"Customer :\")\n",
    "            end = text.index(\"Agent :\")\n",
    "            customer_text = text[start:end].strip()\n",
    "            return customer_text\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def create_xy_pairs(df):\n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Extract fields from the CSV columns\n",
    "            situation = str(row.get('Situation', '')).strip()\n",
    "            emotion = str(row.get('emotion', 'neutral')).strip()\n",
    "            customer = extract_customer_utterance(row.get('empathetic_dialogues', ''))\n",
    "            agent = str(row.get('labels', '')).strip()\n",
    "            \n",
    "            # Skip if any essential field is missing\n",
    "            if not customer or not agent:\n",
    "                continue\n",
    "            \n",
    "            # Format input according to requirements\n",
    "            x_text = f\"Emotion: {emotion} | Situation: {situation} | Customer: {customer} Agent:\"\n",
    "            y_text = agent\n",
    "            \n",
    "            X_list.append(x_text)\n",
    "            Y_list.append(y_text)\n",
    "        \n",
    "        return X_list, Y_list\n",
    "    \n",
    "    train_X, train_Y = create_xy_pairs(train_df)\n",
    "    val_X, val_Y = create_xy_pairs(val_df)\n",
    "    test_X, test_Y = create_xy_pairs(test_df)\n",
    "    \n",
    "    print(f\"Training pairs: {len(train_X)}\")\n",
    "    print(f\"Validation pairs: {len(val_X)}\")\n",
    "    print(f\"Test pairs: {len(test_X)}\")\n",
    "    print(f\"\\nExample input X:\")\n",
    "    print(f\"{train_X[0][:200]}...\")\n",
    "    print(f\"\\nExample output Y:\")\n",
    "    print(f\"{train_Y[0]}\")\n",
    "    \n",
    "    return train_X, train_Y, val_X, val_Y, test_X, test_Y\n",
    "\n",
    "# Prepare data\n",
    "train_X, train_Y, val_X, val_Y, test_X, test_Y = prepare_data(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee30489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from training data only\n",
    "vocab = Vocabulary(\n",
    "    pad_token=config.PAD_TOKEN,\n",
    "    bos_token=config.BOS_TOKEN,\n",
    "    eos_token=config.EOS_TOKEN,\n",
    "    unk_token=config.UNK_TOKEN\n",
    ")\n",
    "\n",
    "# Combine all training texts to build vocabulary\n",
    "all_train_texts = train_X + train_Y\n",
    "vocab.build_vocabulary(all_train_texts, min_freq=config.MIN_FREQ, max_vocab_size=config.MAX_VOCAB_SIZE)\n",
    "\n",
    "# Test encoding/decoding\n",
    "sample_text = train_Y[0]\n",
    "encoded = vocab.encode(sample_text)\n",
    "decoded = vocab.decode(encoded)\n",
    "print(f\"\\nOriginal: {sample_text}\")\n",
    "print(f\"Encoded: {encoded[:20]}...\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data format - show a few examples\n",
    "print(\"=\"*80)\n",
    "print(\"DATA VERIFICATION - Sample Input/Output Pairs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(train_X))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"INPUT (X):\\n{train_X[i]}\")\n",
    "    print(f\"\\nOUTPUT (Y):\\n{train_Y[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c86bb",
   "metadata": {},
   "source": [
    "## 4. PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d03ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpatheticDataset(Dataset):\n",
    "    \"\"\"Dataset for empathetic dialogues\"\"\"\n",
    "    \n",
    "    def __init__(self, X_texts, Y_texts, vocab, max_len=128):\n",
    "        self.X_texts = X_texts\n",
    "        self.Y_texts = Y_texts\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_text = self.X_texts[idx]\n",
    "        y_text = self.Y_texts[idx]\n",
    "        \n",
    "        # Encode texts\n",
    "        x_encoded = self.vocab.encode(x_text, add_special_tokens=True)\n",
    "        y_encoded = self.vocab.encode(y_text, add_special_tokens=True)\n",
    "        \n",
    "        # Truncate if needed\n",
    "        x_encoded = x_encoded[:self.max_len]\n",
    "        y_encoded = y_encoded[:self.max_len]\n",
    "        \n",
    "        return {\n",
    "            'encoder_input': torch.tensor(x_encoded, dtype=torch.long),\n",
    "            'decoder_input': torch.tensor(y_encoded[:-1], dtype=torch.long),  # Remove last token\n",
    "            'decoder_target': torch.tensor(y_encoded[1:], dtype=torch.long)   # Remove first token (BOS)\n",
    "        }\n",
    "\n",
    "print(\"EmpatheticDataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c806a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to pad sequences in a batch\"\"\"\n",
    "    encoder_inputs = [item['encoder_input'] for item in batch]\n",
    "    decoder_inputs = [item['decoder_input'] for item in batch]\n",
    "    decoder_targets = [item['decoder_target'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    encoder_inputs_padded = pad_sequence(encoder_inputs, batch_first=True, padding_value=0)\n",
    "    decoder_inputs_padded = pad_sequence(decoder_inputs, batch_first=True, padding_value=0)\n",
    "    decoder_targets_padded = pad_sequence(decoder_targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'encoder_input': encoder_inputs_padded,\n",
    "        'decoder_input': decoder_inputs_padded,\n",
    "        'decoder_target': decoder_targets_padded\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmpatheticDataset(train_X, train_Y, vocab, max_len=config.MAX_SEQ_LEN)\n",
    "val_dataset = EmpatheticDataset(val_X, val_Y, vocab, max_len=config.MAX_SEQ_LEN)\n",
    "test_dataset = EmpatheticDataset(test_X, test_Y, vocab, max_len=config.MAX_SEQ_LEN)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, \n",
    "                         shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, \n",
    "                       shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, \n",
    "                        shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  Encoder input: {sample_batch['encoder_input'].shape}\")\n",
    "print(f\"  Decoder input: {sample_batch['decoder_input'].shape}\")\n",
    "print(f\"  Decoder target: {sample_batch['decoder_target'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f58f2c",
   "metadata": {},
   "source": [
    "## 5. Transformer Architecture from Scratch\n",
    "\n",
    "We'll implement all components:\n",
    "- Positional Encoding\n",
    "- Multi-Head Attention\n",
    "- Feed-Forward Network\n",
    "- Encoder Layer\n",
    "- Decoder Layer\n",
    "- Full Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(\"PositionalEncoding defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q, K, V: (batch_size, num_heads, seq_len, d_k)\n",
    "            mask: (batch_size, 1, seq_len, seq_len) or (batch_size, 1, 1, seq_len)\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query, key, value: (batch_size, seq_len, d_model)\n",
    "            mask: (batch_size, seq_len, seq_len) for decoder or (batch_size, seq_len) for padding\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections and split into heads\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        x, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"MultiHeadAttention defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "print(\"FeedForward defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Transformer Encoder Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "            mask: (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"EncoderLayer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Transformer Decoder Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, tgt_seq_len, d_model)\n",
    "            encoder_output: (batch_size, src_seq_len, d_model)\n",
    "            src_mask: mask for encoder output\n",
    "            tgt_mask: causal mask for decoder (batch_size, tgt_seq_len, tgt_seq_len)\n",
    "        \"\"\"\n",
    "        # Masked self-attention with residual connection\n",
    "        attn_output, _ = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Cross-attention with encoder output and residual connection\n",
    "        attn_output, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"DecoderLayer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb46d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete Transformer Encoder-Decoder Model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=4, \n",
    "                 num_encoder_layers=2, num_decoder_layers=2, \n",
    "                 d_ff=512, dropout=0.1, max_seq_len=128, pad_idx=0):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Embeddings\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"Create mask for source sequence (padding mask)\"\"\"\n",
    "        # src: (batch_size, src_len)\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (batch_size, 1, 1, src_len)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self, tgt):\n",
    "        \"\"\"Create causal mask for target sequence\"\"\"\n",
    "        # tgt: (batch_size, tgt_len)\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        \n",
    "        # Padding mask\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (batch_size, 1, 1, tgt_len)\n",
    "        \n",
    "        # Causal mask (no future information)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        # (tgt_len, tgt_len)\n",
    "        \n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "        # (batch_size, 1, tgt_len, tgt_len)\n",
    "        \n",
    "        return tgt_mask\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"Encoder forward pass\"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"Decoder forward pass\"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_len)\n",
    "            tgt: (batch_size, tgt_len)\n",
    "        Returns:\n",
    "            output: (batch_size, tgt_len, vocab_size)\n",
    "        \"\"\"\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        \n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.fc_out(decoder_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"Transformer model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01c18b",
   "metadata": {},
   "source": [
    "## 6. Model Instantiation and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Transformer(\n",
    "    vocab_size=len(vocab),\n",
    "    d_model=config.EMBEDDING_DIM,\n",
    "    num_heads=config.NUM_HEADS,\n",
    "    num_encoder_layers=config.NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=config.NUM_DECODER_LAYERS,\n",
    "    d_ff=config.FFN_DIM,\n",
    "    dropout=config.DROPOUT,\n",
    "    max_seq_len=config.MAX_SEQ_LEN,\n",
    "    pad_idx=vocab.token2idx[config.PAD_TOKEN]\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Optimizer - Adam with betas=(0.9, 0.98)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Loss function - CrossEntropyLoss with padding ignored\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.token2idx[config.PAD_TOKEN])\n",
    "\n",
    "print(f\"\\nModel initialized on {device}\")\n",
    "print(f\"Optimizer: Adam (lr={config.LEARNING_RATE}, betas=(0.9, 0.98))\")\n",
    "print(f\"Loss function: CrossEntropyLoss (ignore padding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0038b",
   "metadata": {},
   "source": [
    "## 7. Training with Teacher Forcing\n",
    "\n",
    "Training loop with:\n",
    "- Teacher forcing (use ground truth as decoder input)\n",
    "- Gradient clipping\n",
    "- Validation after each epoch\n",
    "- Save best model based on validation BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device, clip_grad=1.0):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        src = batch['encoder_input'].to(device)\n",
    "        tgt_input = batch['decoder_input'].to(device)\n",
    "        tgt_output = batch['decoder_target'].to(device)\n",
    "        \n",
    "        # Forward pass with teacher forcing\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, tgt_output)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_loss(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate loss on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch['encoder_input'].to(device)\n",
    "            tgt_input = batch['decoder_input'].to(device)\n",
    "            tgt_output = batch['decoder_target'].to(device)\n",
    "            \n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc084378",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics\n",
    "\n",
    "Implementation of:\n",
    "- BLEU score\n",
    "- ROUGE-L\n",
    "- chrF (character n-gram F-score)\n",
    "- Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(references, hypotheses):\n",
    "    \"\"\"Calculate corpus-level BLEU score\"\"\"\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    scores = []\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ref_tokens = ref.split()\n",
    "        hyp_tokens = hyp.split()\n",
    "        score = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth_fn)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "def calculate_rouge_l(references, hypotheses):\n",
    "    \"\"\"Calculate ROUGE-L score\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = []\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        score = scorer.score(ref, hyp)\n",
    "        scores.append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "def calculate_chrf(references, hypotheses):\n",
    "    \"\"\"Calculate chrF score (character n-gram F-score)\"\"\"\n",
    "    def char_ngrams(text, n=3):\n",
    "        \"\"\"Get character n-grams\"\"\"\n",
    "        chars = list(text.replace(' ', ''))\n",
    "        return [tuple(chars[i:i+n]) for i in range(len(chars)-n+1)]\n",
    "    \n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ref_ngrams = set(char_ngrams(ref))\n",
    "        hyp_ngrams = set(char_ngrams(hyp))\n",
    "        \n",
    "        if not hyp_ngrams:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        intersection = len(ref_ngrams & hyp_ngrams)\n",
    "        precision = intersection / len(hyp_ngrams) if hyp_ngrams else 0\n",
    "        recall = intersection / len(ref_ngrams) if ref_ngrams else 0\n",
    "        \n",
    "        if precision + recall > 0:\n",
    "            f_score = 2 * precision * recall / (precision + recall)\n",
    "        else:\n",
    "            f_score = 0.0\n",
    "        \n",
    "        scores.append(f_score)\n",
    "    \n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    \"\"\"Calculate perplexity from loss\"\"\"\n",
    "    return math.exp(loss)\n",
    "\n",
    "print(\"Evaluation metrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab91be5",
   "metadata": {},
   "source": [
    "## 9. Inference Functions\n",
    "\n",
    "Implementation of:\n",
    "- Greedy decoding\n",
    "- Beam search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, vocab, max_len=50, device='cpu'):\n",
    "    \"\"\"Greedy decoding for inference\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode source\n",
    "        src = src.to(device)\n",
    "        src_mask = model.make_src_mask(src)\n",
    "        encoder_output = model.encode(src, src_mask)\n",
    "        \n",
    "        # Start with BOS token\n",
    "        tgt_indices = [vocab.token2idx[vocab.bos_token]]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            tgt = torch.LongTensor([tgt_indices]).to(device)\n",
    "            tgt_mask = model.make_tgt_mask(tgt)\n",
    "            \n",
    "            # Decode\n",
    "            decoder_output = model.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "            output = model.fc_out(decoder_output)\n",
    "            \n",
    "            # Get next token (greedy)\n",
    "            next_token = output[0, -1, :].argmax().item()\n",
    "            tgt_indices.append(next_token)\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if next_token == vocab.token2idx[vocab.eos_token]:\n",
    "                break\n",
    "        \n",
    "        return tgt_indices\n",
    "\n",
    "def beam_search_decode(model, src, vocab, beam_width=3, max_len=50, device='cpu'):\n",
    "    \"\"\"Beam search decoding for inference\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode source\n",
    "        src = src.to(device)\n",
    "        src_mask = model.make_src_mask(src)\n",
    "        encoder_output = model.encode(src, src_mask)\n",
    "        \n",
    "        # Initialize beams: [(sequence, score)]\n",
    "        beams = [([vocab.token2idx[vocab.bos_token]], 0.0)]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            candidates = []\n",
    "            \n",
    "            for seq, score in beams:\n",
    "                # Skip if sequence already ended\n",
    "                if seq[-1] == vocab.token2idx[vocab.eos_token]:\n",
    "                    candidates.append((seq, score))\n",
    "                    continue\n",
    "                \n",
    "                tgt = torch.LongTensor([seq]).to(device)\n",
    "                tgt_mask = model.make_tgt_mask(tgt)\n",
    "                \n",
    "                # Decode\n",
    "                decoder_output = model.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "                output = model.fc_out(decoder_output)\n",
    "                \n",
    "                # Get top-k tokens\n",
    "                log_probs = F.log_softmax(output[0, -1, :], dim=0)\n",
    "                top_k_probs, top_k_indices = torch.topk(log_probs, beam_width)\n",
    "                \n",
    "                # Create new candidates\n",
    "                for prob, idx in zip(top_k_probs, top_k_indices):\n",
    "                    new_seq = seq + [idx.item()]\n",
    "                    new_score = score + prob.item()\n",
    "                    candidates.append((new_seq, new_score))\n",
    "            \n",
    "            # Keep top beam_width candidates\n",
    "            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            \n",
    "            # Check if all beams ended\n",
    "            if all(seq[-1] == vocab.token2idx[vocab.eos_token] for seq, _ in beams):\n",
    "                break\n",
    "        \n",
    "        # Return best sequence\n",
    "        best_seq, best_score = beams[0]\n",
    "        return best_seq\n",
    "\n",
    "def generate_response(model, input_text, vocab, device='cpu', method='greedy', beam_width=3):\n",
    "    \"\"\"Generate response for given input text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode input\n",
    "    src_indices = vocab.encode(input_text, add_special_tokens=True)\n",
    "    src = torch.LongTensor([src_indices])\n",
    "    \n",
    "    # Decode\n",
    "    if method == 'greedy':\n",
    "        output_indices = greedy_decode(model, src, vocab, device=device)\n",
    "    elif method == 'beam':\n",
    "        output_indices = beam_search_decode(model, src, vocab, beam_width=beam_width, device=device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Decode to text\n",
    "    output_text = vocab.decode(output_indices, skip_special_tokens=True)\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "print(\"Inference functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a62c7",
   "metadata": {},
   "source": [
    "## 10. Main Training Loop\n",
    "\n",
    "Train the model with validation and save best checkpoint based on BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, vocab, device, num_samples=100):\n",
    "    \"\"\"Evaluate model and calculate metrics\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    count = 0\n",
    "    for batch in dataloader:\n",
    "        if count >= num_samples:\n",
    "            break\n",
    "        \n",
    "        src = batch['encoder_input'].to(device)\n",
    "        tgt = batch['decoder_target'].to(device)\n",
    "        \n",
    "        batch_size = src.size(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "            \n",
    "            # Get reference\n",
    "            ref_indices = tgt[i].cpu().tolist()\n",
    "            ref_text = vocab.decode(ref_indices, skip_special_tokens=True)\n",
    "            \n",
    "            # Generate hypothesis\n",
    "            src_i = src[i:i+1]\n",
    "            hyp_indices = greedy_decode(model, src_i, vocab, device=device)\n",
    "            hyp_text = vocab.decode(hyp_indices, skip_special_tokens=True)\n",
    "            \n",
    "            references.append(ref_text)\n",
    "            hypotheses.append(hyp_text)\n",
    "            count += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu = calculate_bleu(references, hypotheses)\n",
    "    rouge_l = calculate_rouge_l(references, hypotheses)\n",
    "    chrf = calculate_chrf(references, hypotheses)\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu,\n",
    "        'rouge_l': rouge_l,\n",
    "        'chrf': chrf,\n",
    "        'references': references,\n",
    "        'hypotheses': hypotheses\n",
    "    }\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_bleu': [],\n",
    "    'val_perplexity': []\n",
    "}\n",
    "\n",
    "best_bleu = 0.0\n",
    "best_model_path = 'best_transformer_model.pt'\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device, config.GRAD_CLIP)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate_loss(model, val_loader, criterion, device)\n",
    "    val_perplexity = calculate_perplexity(val_loss)\n",
    "    \n",
    "    # Evaluate on validation set (sample)\n",
    "    val_metrics = evaluate_model(model, val_loader, vocab, device, num_samples=50)\n",
    "    val_bleu = val_metrics['bleu']\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_bleu'].append(val_bleu)\n",
    "    history['val_perplexity'].append(val_perplexity)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Perplexity: {val_perplexity:.4f}\")\n",
    "    print(f\"  Val BLEU: {val_bleu:.4f}\")\n",
    "    \n",
    "    # Save best model based on BLEU score\n",
    "    if val_bleu > best_bleu:\n",
    "        best_bleu = val_bleu\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_bleu': val_bleu,\n",
    "            'config': config\n",
    "        }, best_model_path)\n",
    "        print(f\"   Best model saved! (BLEU: {val_bleu:.4f})\")\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation BLEU: {best_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d95d3e",
   "metadata": {},
   "source": [
    "## 11. Test Set Evaluation\n",
    "\n",
    "Comprehensive evaluation on test set with all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb07c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss = evaluate_loss(model, test_loader, criterion, device)\n",
    "test_perplexity = calculate_perplexity(test_loss)\n",
    "\n",
    "test_metrics = evaluate_model(model, test_loader, vocab, device, num_samples=200)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Perplexity: {test_perplexity:.4f}\")\n",
    "print(f\"Test BLEU: {test_metrics['bleu']:.4f}\")\n",
    "print(f\"Test ROUGE-L: {test_metrics['rouge_l']:.4f}\")\n",
    "print(f\"Test chrF: {test_metrics['chrf']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2aff5",
   "metadata": {},
   "source": [
    "## 12. Qualitative Examples\n",
    "\n",
    "Compare model outputs with ground truth responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show qualitative examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUALITATIVE EXAMPLES - Model vs Ground Truth\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "num_examples = 10\n",
    "for i in range(min(num_examples, len(test_X))):\n",
    "    input_text = test_X[i]\n",
    "    ground_truth = test_Y[i]\n",
    "    \n",
    "    # Generate with greedy decoding\n",
    "    model_output_greedy = generate_response(model, input_text, vocab, device, method='greedy')\n",
    "    \n",
    "    # Generate with beam search\n",
    "    model_output_beam = generate_response(model, input_text, vocab, device, method='beam', beam_width=3)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input: {input_text[:150]}...\")\n",
    "    print(f\"\\nGround Truth: {ground_truth}\")\n",
    "    print(f\"Model (Greedy): {model_output_greedy}\")\n",
    "    print(f\"Model (Beam-3): {model_output_beam}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3000ea",
   "metadata": {},
   "source": [
    "## 13. Interactive Testing\n",
    "\n",
    "Test the model with custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom inputs\n",
    "def test_chatbot(emotion, situation, customer_utterance, method='greedy'):\n",
    "    \"\"\"Test the chatbot with custom input\"\"\"\n",
    "    input_text = f\"Emotion: {emotion} | Situation: {situation} | Customer: {customer_utterance} Agent:\"\n",
    "    response = generate_response(model, input_text, vocab, device, method=method)\n",
    "    return response\n",
    "\n",
    "# Example 1: Sentimental\n",
    "print(\"=\"*80)\n",
    "print(\"Example 1: Sentimental\")\n",
    "emotion = \"sentimental\"\n",
    "situation = \"I remember going to the fireworks with my best friend\"\n",
    "customer = \"This was a best friend. I miss her.\"\n",
    "response = test_chatbot(emotion, situation, customer, method='beam')\n",
    "print(f\"Emotion: {emotion}\")\n",
    "print(f\"Situation: {situation}\")\n",
    "print(f\"Customer: {customer}\")\n",
    "print(f\"Agent: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example 2: Afraid\")\n",
    "emotion = \"afraid\"\n",
    "situation = \"I used to scare for darkness\"\n",
    "customer = \"it feels like hitting to blank wall when I see the darkness\"\n",
    "response = test_chatbot(emotion, situation, customer, method='beam')\n",
    "print(f\"Emotion: {emotion}\")\n",
    "print(f\"Situation: {situation}\")\n",
    "print(f\"Customer: {customer}\")\n",
    "print(f\"Agent: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example 3: Joyful\")\n",
    "emotion = \"joyful\"\n",
    "situation = \"I got promoted at work today\"\n",
    "customer = \"I am so happy about this news!\"\n",
    "response = test_chatbot(emotion, situation, customer, method='beam')\n",
    "print(f\"Emotion: {emotion}\")\n",
    "print(f\"Situation: {situation}\")\n",
    "print(f\"Customer: {customer}\")\n",
    "print(f\"Agent: {response}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1da316",
   "metadata": {},
   "source": [
    "## 14. Save Model and Vocabulary\n",
    "\n",
    "Save the final model and vocabulary for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbee282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary\n",
    "vocab_data = {\n",
    "    'token2idx': vocab.token2idx,\n",
    "    'idx2token': vocab.idx2token,\n",
    "    'pad_token': vocab.pad_token,\n",
    "    'bos_token': vocab.bos_token,\n",
    "    'eos_token': vocab.eos_token,\n",
    "    'unk_token': vocab.unk_token\n",
    "}\n",
    "\n",
    "import pickle\n",
    "with open('vocabulary.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_data, f)\n",
    "\n",
    "print(\"Vocabulary saved to vocabulary.pkl\")\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {\n",
    "        'vocab_size': len(vocab),\n",
    "        'd_model': config.EMBEDDING_DIM,\n",
    "        'num_heads': config.NUM_HEADS,\n",
    "        'num_encoder_layers': config.NUM_ENCODER_LAYERS,\n",
    "        'num_decoder_layers': config.NUM_DECODER_LAYERS,\n",
    "        'd_ff': config.FFN_DIM,\n",
    "        'dropout': config.DROPOUT,\n",
    "        'max_seq_len': config.MAX_SEQ_LEN,\n",
    "        'pad_idx': vocab.token2idx[config.PAD_TOKEN]\n",
    "    },\n",
    "    'test_metrics': {\n",
    "        'bleu': test_metrics['bleu'],\n",
    "        'rouge_l': test_metrics['rouge_l'],\n",
    "        'chrf': test_metrics['chrf'],\n",
    "        'perplexity': test_perplexity\n",
    "    }\n",
    "}, 'final_transformer_model.pt')\n",
    "\n",
    "print(\"Final model saved to final_transformer_model.pt\")\n",
    "print(\"\\nAll files ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713455b",
   "metadata": {},
   "source": [
    "## 15. Summary and Next Steps\n",
    "\n",
    "### What We Built:\n",
    " **Transformer encoder-decoder from scratch** (no pretrained weights)\n",
    "- Multi-head attention\n",
    "- Positional encoding\n",
    "- Feed-forward networks\n",
    "- Layer normalization\n",
    "- Residual connections\n",
    "\n",
    " **Complete preprocessing pipeline**\n",
    "- Text normalization (lowercase, whitespace, punctuation)\n",
    "- Custom vocabulary built from training data only\n",
    "- Special tokens: `<pad>`, `<bos>`, `<eos>`, `<unk>`\n",
    "- Train/Val/Test split (80/10/10)\n",
    "\n",
    " **Training with teacher forcing**\n",
    "- Adam optimizer (betas=0.9, 0.98)\n",
    "- Gradient clipping\n",
    "- Best model saved based on validation BLEU\n",
    "\n",
    " **Comprehensive evaluation**\n",
    "- BLEU score\n",
    "- ROUGE-L\n",
    "- chrF (character n-gram F-score)\n",
    "- Perplexity\n",
    "\n",
    " **Inference strategies**\n",
    "- Greedy decoding\n",
    "- Beam search decoding\n",
    "\n",
    " **Causal masking in decoder** (no future token access)\n",
    "\n",
    "### Model Architecture:\n",
    "- Embedding dimension: 256\n",
    "- Attention heads: 4\n",
    "- Encoder layers: 2\n",
    "- Decoder layers: 2\n",
    "- Feed-forward dimension: 512\n",
    "- Dropout: 0.1\n",
    "\n",
    "### Next Steps for Deployment:\n",
    "\n",
    "1. **Streamlit/Gradio UI**: Create interactive chatbot interface\n",
    "2. **Deploy**: Host on Streamlit Cloud or Gradio public link\n",
    "3. **Evaluation Report**: Document metrics and qualitative examples\n",
    "4. **Blog Post**: Write Medium article explaining the approach\n",
    "\n",
    "### Files Generated:\n",
    "- `best_transformer_model.pt` - Best model checkpoint (based on validation BLEU)\n",
    "- `final_transformer_model.pt` - Final trained model\n",
    "- `vocabulary.pkl` - Vocabulary for tokenization\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This notebook implements all requirements from scratch. On Kaggle, ensure:\n",
    "- Dataset path is correct (`/kaggle/input/empathetic-dialogues-facebook-ai/`)\n",
    "- GPU is enabled for faster training\n",
    "- Adjust `NUM_EPOCHS` and `BATCH_SIZE` based on available resources"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
